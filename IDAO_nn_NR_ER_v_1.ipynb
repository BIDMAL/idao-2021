{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azAO0fJdqDE6"
   },
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5L__XKNl8PPJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gN4sqzGEA-eG",
    "outputId": "a96646ea-a502-4219-c98e-26498015fa36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DATA_MODES = ['train', 'val', 'test']\n",
    "RESCALE_SIZE = 224\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlOxz93IBhBC"
   },
   "source": [
    "# Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZMhU0i1sYtt"
   },
   "source": [
    "## Модель `create_nn_v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "drjr84ONDd5M"
   },
   "outputs": [],
   "source": [
    "data_dir = './data/classed_type_train/'\n",
    "test_dir = './data/test2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Rng9f-4qFJm5"
   },
   "outputs": [],
   "source": [
    "#Набор трансформаций для обучающей выборки\n",
    "train_transforms = transforms.Compose([transforms.CenterCrop(224), \n",
    "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                       transforms.RandomVerticalFlip(p=0.5),\n",
    "                                       transforms.RandomRotation(degrees=(45, 90)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           [0.485, 0.456, 0.406],\n",
    "                                            [0.229, 0.224, 0.225])\n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4NhX-_IUFgPx"
   },
   "outputs": [],
   "source": [
    "#Набор трансформаций для валидационной выборки\n",
    "valid_transforms = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           [0.485, 0.456, 0.406],\n",
    "                                           [0.229, 0.224, 0.225])\n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E1ETYBjuFfjt"
   },
   "outputs": [],
   "source": [
    "#Набор трансформаций для тестовой выборки\n",
    "test_transforms = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(\n",
    "                                          [0.485, 0.456, 0.406],\n",
    "                                           [0.229, 0.224, 0.225])\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "s5bktRR8tCbO"
   },
   "outputs": [],
   "source": [
    "#Объявляю функцию загрузчика с аргументами:\n",
    "#Директория, Размер батча, Набор трансформаций для тренировочной выборки,\n",
    "#Набор трансформаций для валидационной выборки, Размер валидационной выборки\n",
    "def load_split_train_valid(datadir,\n",
    "                           batch_size,\n",
    "                           train_transforms,\n",
    "                           valid_transforms,\n",
    "                           valid_size): \n",
    "    \n",
    "    #Загрузчик для тренировочной выборки\n",
    "    train_data = datasets.ImageFolder(datadir,       \n",
    "                    transform=train_transforms)\n",
    "    #Загрузчик для тренировочной выборки\n",
    "    test_data = datasets.ImageFolder(datadir,\n",
    "                    transform=valid_transforms)\n",
    "    #Код для разделения на трейн и тест в указанном соотношении\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=batch_size)\n",
    "    testloader = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=batch_size)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMV_UDtxtQhS",
    "outputId": "5d0ad4cc-85fc-49af-ca2b-b29638526c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ER', 'NR']\n",
      "['ER', 'NR']\n",
      "163\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "#Получаю тренировочный и валидационный генераторы\n",
    "train_loader, val_loader = load_split_train_valid(datadir=data_dir,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           train_transforms=train_transforms,\n",
    "                           valid_transforms=valid_transforms,\n",
    "                           valid_size = .2)\n",
    "#Проверяю результаты работы генераторов\n",
    "print(train_loader.dataset.classes)\n",
    "print(val_loader.dataset.classes)\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HehuufC8EQvY"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): \n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "            self.c1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "            self.c2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "            self.c3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "            self.fc1 = nn.Linear(in_features=6272, out_features=1024)\n",
    "            self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "            self.fc3 = nn.Linear(in_features=512, out_features=256)\n",
    "            self.fc4 = nn.Linear(in_features=256, out_features=6)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.c1(x), 3))\n",
    "            x = F.relu(F.max_pool2d(self.c2(x), 3))\n",
    "            x = F.relu(F.max_pool2d(self.c3(x), 3))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "\n",
    "            return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c8Aj6fnEQ-7",
    "outputId": "a9eef15f-f955-468d-c72a-44fe1d4a666c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (c1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (c2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (c3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=6272, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "print(net)\n",
    "#net = net.cuda()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyHS2dE7Eh9A",
    "outputId": "d5e23adf-2410-44ce-998d-285dd7f8d460"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/163], Loss: 0.6126, Accuracy: 62.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▏                                                                         | 1/10 [01:47<16:06, 107.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [100/163], Loss: 0.2487, Accuracy: 89.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [02:33<09:31, 71.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [100/163], Loss: 0.0814, Accuracy: 93.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [03:19<06:58, 59.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [100/163], Loss: 0.0050, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [04:05<05:27, 54.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [100/163], Loss: 0.0031, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [04:52<04:18, 51.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [100/163], Loss: 0.0124, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [05:40<03:22, 50.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [100/163], Loss: 0.0045, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [06:30<02:30, 50.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [100/163], Loss: 0.0048, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [07:20<01:40, 50.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [100/163], Loss: 0.0138, Accuracy: 98.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [08:10<00:50, 50.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [100/163], Loss: 0.0008, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [08:59<00:00, 53.94s/it]\n"
     ]
    }
   ],
   "source": [
    "#Задаю функцию потерь\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#Задаю оптимизатор\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "    #Прямой запуск\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "            \n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        #Обратное распространение и оптимизатор\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Отслеживание точности\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            .format(epoch + 1, EPOCHS, i + 1, total_step, loss.item(),\n",
    "                                  (correct / total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCjSokZcG9cy",
    "outputId": "333cf48e-e982-446a-d6b7-ef31bc99364b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [00:25<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 99.38295410721172 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Проверка на валидационной выборке\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(val_loader):\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy of the model on the test images: {} %'\n",
    "              .format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.ImageFolder(test_dir,\n",
    "                transform=test_transforms)\n",
    "\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "filenames = [line[0].split('\\\\')[1].split('.')[0] for line in test_data.imgs]\n",
    "pred2class = [0 if n == 'ER' else 1 for n in train_loader.dataset.classes]\n",
    "dict_pred = defaultdict(list)\n",
    "dict_pred[\"id\"] = filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (img, name) in enumerate(iter(test_loader)):\n",
    "        img = img.to(device)\n",
    "        outputs = net(img)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = list(map(int, predicted.cpu()))\n",
    "        dict_pred[\"classification_predictions\"].extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16564\n",
      "16564\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_pred['id']))\n",
    "print(len(dict_pred['classification_predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(dict_pred, columns=[\"id\", \"classification_predictions\"])\n",
    "data_frame.to_csv('./submissions/submission_tryCNN_cls.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IDAO_nn_NR_ER_v_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
